{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54196e7e-1f1f-4228-997c-2fedb856bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "print('this is line 5 of necrozma.py')\n",
    "from .fedbase import BaseFedarated\n",
    "from flearn.utils.tf_utils import process_grad, cosine_sim, softmax, norm_grad, l2_clip, get_stdev\n",
    "from flearn.utils.model_utils import batch_data, gen_batch, gen_epoch, gen_batch_celeba\n",
    "from flearn.utils.language_utils import letter_to_vec, word_to_indices\n",
    "\n",
    "\n",
    "def process_x(raw_x_batch):\n",
    "    x_batch = [word_to_indices(word) for word in raw_x_batch]\n",
    "    x_batch = np.array(x_batch)\n",
    "    return x_batch\n",
    "\n",
    "def process_y(raw_y_batch):\n",
    "    y_batch = [letter_to_vec(c) for c in raw_y_batch]\n",
    "    return y_batch\n",
    "\n",
    "\n",
    "class Server(BaseFedarated):\n",
    "    def __init__(self, params, learner, dataset):\n",
    "        print('Using global-regularized multi-task learning to Train')\n",
    "        self.inner_opt = tf.train.GradientDescentOptimizer(params['learning_rate'])\n",
    "        super(Server, self).__init__(params, learner, dataset)\n",
    "\n",
    "    def train(self):\n",
    "        print('---{} workers per communication round---'.format(self.clients_per_round))\n",
    "\n",
    "        np.random.seed(1234567+self.seed)\n",
    "        corrupt_id = np.random.choice(range(len(self.clients)), size=self.num_corrupted, replace=False)\n",
    "        print(corrupt_id)\n",
    "\n",
    "        if self.dataset == 'shakespeare':\n",
    "            for c in self.clients:\n",
    "                c.train_data['y'], c.train_data['x'] = process_y(c.train_data['y']), process_x(c.train_data['x'])\n",
    "                c.test_data['y'], c.test_data['x'] = process_y(c.test_data['y']), process_x(c.test_data['x'])\n",
    "\n",
    "        batches = {}\n",
    "        for idx, c in enumerate(self.clients):\n",
    "            if idx in corrupt_id:\n",
    "                c.train_data['y'] = np.asarray(c.train_data['y'])\n",
    "                if self.dataset == 'celeba':\n",
    "                    c.train_data['y'] = 1 - c.train_data['y']\n",
    "                elif self.dataset == 'femnist':\n",
    "                    c.train_data['y'] = np.random.randint(0, 62, len(c.train_data['y']))  # [0, 62)\n",
    "                elif self.dataset == 'shakespeare':\n",
    "                    c.train_data['y'] = np.random.randint(0, 80, len(c.train_data['y']))\n",
    "                elif self.dataset == \"vehicle\":\n",
    "                    c.train_data['y'] = c.train_data['y'] * -1\n",
    "                elif self.dataset == \"fmnist\":\n",
    "                    c.train_data['y'] = np.random.randint(0, 10, len(c.train_data['y']))\n",
    "\n",
    "            if self.dataset == 'celeba':\n",
    "                # due to a different data storage format\n",
    "                batches[c] = gen_batch_celeba(c.train_data, self.batch_size, self.num_rounds * self.local_iters)\n",
    "            else:\n",
    "                batches[c] = gen_batch(c.train_data, self.batch_size, self.num_rounds * self.local_iters)\n",
    "\n",
    "        Necrozma_accuracies=[]\n",
    "        Necrozma_var=[]\n",
    "        \n",
    "        for i in range(self.num_rounds + 1):\n",
    "            if i % self.eval_every == 0 and i > 0:\n",
    "                tmp_models = [self.local_models[idx] for idx in range(len(self.clients))]\n",
    "\n",
    "                # num_train, num_correct_train, loss_vector = self.train_error(tmp_models)\n",
    "                # avg_train_loss = np.dot(loss_vector, num_train) / np.sum(num_train)\n",
    "                # num_test, num_correct_test, _ = self.test(tmp_models)\n",
    "                # tqdm.write('At round {} training accu: {}, loss: {}'.format(i, np.sum(num_correct_train) * 1.0 / np.sum(num_train), avg_train_loss))\n",
    "                # tqdm.write('At round {} test accu: {}'.format(i, np.sum(num_correct_test) * 1.0 / np.sum(num_test)))\n",
    "                # ditto_accuracies.append(np.sum(num_correct_test) * 1.0 / np.sum(num_test))\n",
    "                # non_corrupt_id = np.setdiff1d(range(len(self.clients)), corrupt_id)\n",
    "                # tqdm.write('At round {} malicious test accu: {}'.format(i, np.sum(num_correct_test[corrupt_id]) * 1.0 / np.sum(num_test[corrupt_id])))\n",
    "                # tqdm.write('At round {} benign test accu: {}'.format(i, np.sum(num_correct_test[non_corrupt_id]) * 1.0 / np.sum(num_test[non_corrupt_id])))\n",
    "                # print(\"variance of the performance: \", np.var(num_correct_test[non_corrupt_id] / num_test[non_corrupt_id]))\n",
    "                # ditto_var.append(np.var(num_correct_test[non_corrupt_id] / num_test[non_corrupt_id]))\n",
    "\n",
    "\n",
    "            # weighted sampling\n",
    "            indices, selected_clients = self.select_clients(round=i, num_clients=self.clients_per_round)\n",
    "\n",
    "            csolns = []\n",
    "            losses = []\n",
    "\n",
    "            for idx in indices:\n",
    "                w_global_idx = copy.deepcopy(self.global_model)\n",
    "                c = self.clients[idx] \n",
    "                for _ in range(self.local_iters):\n",
    "                    data_batch = next(batches[c])\n",
    "\n",
    "                    # local\n",
    "                    self.client_model.set_params(self.local_models[idx])\n",
    "                    _, grads, _ = c.solve_sgd(data_batch)  \n",
    "\n",
    "\n",
    "                    if self.dynamic_lam:\n",
    "                        model_tmp = copy.deepcopy(self.local_models[idx])\n",
    "                        model_best = copy.deepcopy(self.local_models[idx])\n",
    "                        tmp_loss = 10000\n",
    "                        # pick a lambda locally based on validation data\n",
    "                        for lam_id, candidate_lam in enumerate([0.1, 1, 2]):\n",
    "                            for layer in range(len(grads[1])):\n",
    "                                eff_grad = grads[1][layer] + candidate_lam * (self.local_models[idx][layer] - self.global_model[layer])\n",
    "                                model_tmp[layer] = self.local_models[idx][layer] - self.learning_rate * eff_grad\n",
    "\n",
    "                            c.set_params(model_tmp)\n",
    "                            l = c.get_val_loss()\n",
    "                            if l < tmp_loss:\n",
    "                                tmp_loss = l\n",
    "                                model_best = copy.deepcopy(model_tmp)\n",
    "\n",
    "                        self.local_models[idx] = copy.deepcopy(model_best)\n",
    "\n",
    "                    else:\n",
    "                        for layer in range(len(grads[1])):\n",
    "                            eff_grad = grads[1][layer] + self.lam * (self.local_models[idx][layer] - self.global_model[layer])\n",
    "                            self.local_models[idx][layer] = self.local_models[idx][layer] - self.learning_rate * eff_grad\n",
    "\n",
    "                    # global\n",
    "                    self.client_model.set_params(w_global_idx)\n",
    "                    loss = c.get_loss() \n",
    "                    losses.append(loss)\n",
    "                    _, grads, _ = c.solve_sgd(data_batch)\n",
    "                    w_global_idx = self.client_model.get_params()\n",
    "\n",
    "\n",
    "                # get the difference (global model updates)\n",
    "                diff = [u - v for (u, v) in zip(w_global_idx, self.global_model)]\n",
    "\n",
    "\n",
    "                # send the malicious updates\n",
    "                if idx in corrupt_id:\n",
    "                    if self.boosting:\n",
    "                        # scale malicious updates\n",
    "                        diff = [self.clients_per_round * u for u in diff]\n",
    "                    elif self.random_updates:\n",
    "                        # send random updates\n",
    "                        stdev_ = get_stdev(diff)\n",
    "                        diff = [np.random.normal(0, stdev_, size=u.shape) for u in diff]\n",
    "\n",
    "                if self.q == 0:\n",
    "                    csolns.append(diff)\n",
    "                else:\n",
    "                    csolns.append((np.exp(self.q * loss), diff))\n",
    "\n",
    "            if self.q != 0:\n",
    "                avg_updates = self.aggregate(csolns)\n",
    "            else:\n",
    "                if self.gradient_clipping:\n",
    "                    csolns = l2_clip(csolns)\n",
    "\n",
    "                expected_num_mali = int(self.clients_per_round * self.num_corrupted / len(self.clients))\n",
    "\n",
    "                if self.median:\n",
    "                    avg_updates = self.median_average(csolns)\n",
    "                elif self.k_norm:\n",
    "                    avg_updates = self.k_norm_average(self.clients_per_round - expected_num_mali, csolns)\n",
    "                elif self.krum:\n",
    "                    avg_updates = self.krum_average(self.clients_per_round - expected_num_mali - 2, csolns)\n",
    "                elif self.mkrum:\n",
    "                    m = self.clients_per_round - expected_num_mali\n",
    "                    avg_updates = self.mkrum_average(self.clients_per_round - expected_num_mali - 2, m, csolns)\n",
    "                else:\n",
    "                    avg_updates = self.simple_average(csolns)\n",
    "\n",
    "            # update the global model\n",
    "            for layer in range(len(avg_updates)):\n",
    "                self.global_model[layer] += avg_updates[layer]\n",
    "        # FedAvg ends here\n",
    "        \n",
    "        tmp_models = []\n",
    "        # Sharing final global model with all clients\n",
    "        for idx in range(len(self.clients)):\n",
    "            c=self.clients[idx]\n",
    "            tmp_models.append(self.local_models[idx])\n",
    "            w_global_idx=copy.deepcopy(self.global_model)\n",
    "            for _ in range(self.local_iters):\n",
    "                data_batch = next(batches[c]) \n",
    "                # global\n",
    "                self.client_model.set_params(w_global_idx)\n",
    "                loss = c.get_loss() \n",
    "                losses.append(loss)\n",
    "                _, grads, _ = c.solve_sgd(data_batch)\n",
    "                w_global_idx = self.client_model.get_params()\n",
    "#             diff = [u - v for (u, v) in zip(w_global_idx, self.global_model)]\n",
    "\n",
    "\n",
    "#             # send the malicious updates\n",
    "#             if idx in corrupt_id:\n",
    "#                 if self.boosting:\n",
    "#                     # scale malicious updates\n",
    "#                     diff = [self.clients_per_round * u for u in diff]\n",
    "#                 elif self.random_updates:\n",
    "#                     # send random updates\n",
    "#                     stdev_ = get_stdev(diff)\n",
    "#                     diff = [np.random.normal(0, stdev_, size=u.shape) for u in diff]\n",
    "#             # To update global parameters**\n",
    "#             if self.q == 0:\n",
    "#                 csolns.append(diff)\n",
    "#             else:\n",
    "#                 csolns.append((np.exp(self.q * loss), diff))\n",
    "\n",
    "#             if self.q != 0:\n",
    "#                 avg_updates = self.aggregate(csolns)\n",
    "#             else:\n",
    "#                 if self.gradient_clipping:\n",
    "#                     csolns = l2_clip(csolns)\n",
    "\n",
    "#                 expected_num_mali = int(self.clients_per_round * self.num_corrupted / len(self.clients))\n",
    "\n",
    "#                 if self.median:\n",
    "#                     avg_updates = self.median_average(csolns)\n",
    "#                 elif self.k_norm:\n",
    "#                     avg_updates = self.k_norm_average(self.clients_per_round - expected_num_mali, csolns)\n",
    "#                 elif self.krum:\n",
    "#                     avg_updates = self.krum_average(self.clients_per_round - expected_num_mali - 2, csolns)\n",
    "#                 elif self.mkrum:\n",
    "#                     m = self.clients_per_round - expected_num_mali\n",
    "#                     avg_updates = self.mkrum_average(self.clients_per_round - expected_num_mali - 2, m, csolns)\n",
    "#                 else:\n",
    "#                     avg_updates = self.simple_average(csolns)\n",
    "        num_train, num_correct_train, loss_vector = self.train_error(tmp_models)\n",
    "        avg_train_loss = np.dot(loss_vector, num_train) / np.sum(num_train)\n",
    "        num_test, num_correct_test, _ = self.test(tmp_models)\n",
    "        tqdm.write('At round {} training accu: {}, loss: {}'.format(i, np.sum(num_correct_train) * 1.0 / np.sum(num_train), avg_train_loss))\n",
    "        tqdm.write('At round {} test accu: {}'.format(i, np.sum(num_correct_test) * 1.0 / np.sum(num_test)))\n",
    "        Necrozma_accuracies.append(np.sum(num_correct_test) * 1.0 / np.sum(num_test))\n",
    "        non_corrupt_id = np.setdiff1d(range(len(self.clients)), corrupt_id)\n",
    "        tqdm.write('At round {} malicious test accu: {}'.format(i, np.sum(num_correct_test[corrupt_id]) * 1.0 / np.sum(num_test[corrupt_id])))\n",
    "        tqdm.write('At round {} benign test accu: {}'.format(i, np.sum(num_correct_test[non_corrupt_id]) * 1.0 / np.sum(num_test[non_corrupt_id])))\n",
    "        print(\"variance of the performance: \", np.var(num_correct_test[non_corrupt_id] / num_test[non_corrupt_id]))\n",
    "        Necrozma_var.append(np.var(num_correct_test[non_corrupt_id] / num_test[non_corrupt_id]))\n",
    "        np.savetxt('Necrozma_acc'+str(int(self.lam*10))+'.txt',Necrozma_accuracies)\n",
    "        np.savetxt('Necrozma_var'+str(int(self.lam*10))+'.txt',Necrozma_var)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
